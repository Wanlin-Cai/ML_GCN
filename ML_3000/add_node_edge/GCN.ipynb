{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将轨道系数导出到orb_coeff.t\n",
    "import os\n",
    "import numpy as np\n",
    "os.chdir('C:/Users/wanli/Desktop/ML/reorganization/dnn-coul/xyz-orb')\n",
    "\n",
    "def orb33(file):\n",
    "    data=np.loadtxt(file)\n",
    "    data =data.tolist()\n",
    "    if len(data) < 33:\n",
    "        for i in range(33-len(data)):\n",
    "            data.append(0)\n",
    "    return data\n",
    "        \n",
    "        \n",
    "outFiles = []\n",
    "for filename in os.listdir('./'): \n",
    "    if filename.endswith('.out'):\n",
    "        outFiles.append(filename)\n",
    "\n",
    "orb_coeff = []\n",
    "for i in range(len(outFiles)):\n",
    "    orb_coeff.append(orb33(outFiles[i]))\n",
    "torch.save(torch.Tensor(orb_coeff),'orb_coeff.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8643bafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/5631\n",
      "Processing molecule 2000/5631\n",
      "Processing molecule 3000/5631\n",
      "Processing molecule 4000/5631\n",
      "Processing molecule 5000/5631\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset,random_split\n",
    "import pandas as pd\n",
    "from rdkit import Chem,DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import ChemicalFeatures\n",
    "from rdkit import RDConfig\n",
    "import os\n",
    "from dgllife.data import MoleculeCSVDataset\n",
    "from functools import partial\n",
    "from dgllife.utils import smiles_to_bigraph, ConsecutiveSplitter\n",
    "\n",
    "# Obtain the features of atoms and bonds\n",
    "def load_coeff(mol):\n",
    "    mol = Chem.MolToSmiles(mol, canonical=True)\n",
    "    df = pd.read_csv('train_set_5631.csv')\n",
    "\n",
    "    sms=[Chem.MolToSmiles(Chem.MolFromSmiles(sm), canonical=True) for sm in df['smiles'].tolist()]\n",
    "    idx=sms.index(mol)\n",
    "    \n",
    "    coeff = torch.load('orb_coeff.t')\n",
    "    return coeff[idx]\n",
    "\n",
    "\n",
    "def featurize_atoms(mol):  \n",
    "    feats = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        hy = [int(atom.GetHybridization()==y) for y in [Chem.rdchem.HybridizationType.SP,\n",
    "              Chem.rdchem.HybridizationType.SP2,Chem.rdchem.HybridizationType.SP3]]\n",
    "        feats.append([atom.GetAtomicNum(), atom.GetExplicitValence(), atom.GetImplicitValence(),\n",
    "                      atom.GetTotalNumHs(), atom.GetFormalCharge(), atom.GetNumRadicalElectrons(),\n",
    "                      atom.GetDegree(), int(atom.GetIsAromatic()), int(atom.IsInRing())]+hy)\n",
    "        \n",
    "    return {'atomic': torch.tensor(feats)}\n",
    "\n",
    "\n",
    "def featurize_edges(mol, add_self_loop=True):     \n",
    "    feats = []\n",
    "    coo = load_coeff(mol)\n",
    "    num_atoms = mol.GetNumAtoms()\n",
    "    for i in range(num_atoms):\n",
    "        for j in range(num_atoms):\n",
    "            e_ij = mol.GetBondBetweenAtoms(i,j)\n",
    "            if e_ij is None:\n",
    "                bond_type = None\n",
    "            else:\n",
    "                bond_type = e_ij.GetBondType()\n",
    "                feats.append([float(bond_type == x)for x in (None, Chem.rdchem.BondType.SINGLE,\n",
    "                              Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC)]\n",
    "                             +[coo[i] * coo[j]])\n",
    "    return {'edgic': torch.tensor(feats)}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df = pd.read_csv('train_set_5631.csv')\n",
    "   \n",
    "    # SMILES to graph-based dataset for prediction model with DGL-Life\n",
    "    dataset=MoleculeCSVDataset(df=df,\n",
    "                               smiles_to_graph=partial(smiles_to_bigraph, add_self_loop=False),\n",
    "                               node_featurizer=featurize_atoms,\n",
    "                               edge_featurizer=None,#featurize_edges,\n",
    "                               smiles_column='smiles',\n",
    "                               cache_file_path='graph.pt',log_every=1000)\n",
    "#    print(dataset)\n",
    "    train_set, val_set, test_set = ConsecutiveSplitter.train_val_test_split(dataset, frac_train=0.8, frac_val=0.1, frac_test=0.1)\n",
    "    torch.save([train_set,val_set,test_set], \"opv_graph-onlyatom.pt\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91670d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0b771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc2111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2aea56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#featurize_atoms(Chem.MolFromSmiles('c1cnc2c(c1)Cc1c2ccc2c3[nH]ccc3[nH]c21'))\n",
    "#featurize_edges(Chem.MolFromSmiles('c1cnc2c(c1)Cc1c2ccc2c3[nH]ccc3[nH]c21'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc5ac2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cef5bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c1cc2[nH]c3c4ncc5ccCc5c4Cc3c2o1',\n",
       " Graph(num_nodes=18, num_edges=44,\n",
       "       ndata_schemes={'atomic': Scheme(shape=(13,), dtype=torch.float32)}\n",
       "       edata_schemes={'edgic': Scheme(shape=(6,), dtype=torch.float32)}),\n",
       " tensor([246.9197]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.load_full = True\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16ba3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56070a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=18, num_edges=44,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "g = smiles_to_bigraph('c1cc2[nH]c3c4ncc5ccCc5c4Cc3c2o1')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304bc5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30dc3e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "torch.Size([44, 6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "www=featurize_edges(Chem.MolFromSmiles('c1cc2[nH]c3c4ncc5ccCc5c4Cc3c2o1'))\n",
    "\n",
    "print([i for i in www.values()][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c0e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a022a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "from dgllife.model.gnn.gat import GAT\n",
    "from dgl.readout import softmax_nodes,sum_nodes\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2, model='Gen'):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = weight_norm(nn.Conv1d(n_inputs, (n_outputs * 2), kernel_size, stride=stride,\n",
    "                                          padding=padding,dilation=dilation))\n",
    "        self.padding = padding\n",
    "        self.model = model\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.trans = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_uniform_(self.conv.weight)\n",
    "        if self.trans is not None:\n",
    "            torch.nn.init.xavier_uniform_(self.trans.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        out = self.glu(y[:, :, :-self.padding].contiguous()) if self.model == 'Gen' else self.glu(y)\n",
    "        out = self.dropout(out)\n",
    "        if self.trans is not None:\n",
    "            x = self.trans(x)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hid_size, n_levels, kernel_size=3, dropout=0.2, model='Gen'):\n",
    "        super(Encoder, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(n_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            padding = (kernel_size - 1) * dilation_size if model == 'Gen' else dilation_size\n",
    "            if i == 0:\n",
    "                layers += [ConvLayer(input_size, hid_size, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=padding, dropout=dropout, model=model)]\n",
    "            else:\n",
    "                layers += [ConvLayer(hid_size, hid_size, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=padding, dropout=dropout, model=model)]\n",
    "        \n",
    "        self.network = (nn.Sequential)(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class NNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_out, hide=(64, 64, 8)):\n",
    "        super(NNet, self).__init__()\n",
    "        self.n_hide = len(hide)\n",
    "        self.fcs = nn.ModuleList([weight_norm(nn.Linear(n_in, hide[i])) if i == 0 else \n",
    "                                  weight_norm(nn.Linear(hide[(i - 1)], n_out)) if i == self.n_hide else \n",
    "                                  weight_norm(nn.Linear(hide[(i - 1)], hide[i])) for i in range(self.n_hide + 1)])\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for i in range(self.n_hide + 1):\n",
    "            self.fcs[i].weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_hide):\n",
    "            x = F.relu(self.fcs[i](x))\n",
    "        x = self.fcs[(-1)](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GEN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_props, dic_size, emb_size, hid_size=256, n_levels=5, kernel_size=3, dropout=0.2):\n",
    "        super(GEN, self).__init__()\n",
    "        self.emb = nn.Embedding(dic_size, emb_size, padding_idx=0)\n",
    "        self.dense0 = NNet(n_props, 16, hide=(8, 64))\n",
    "        self.encoder = Encoder((emb_size + 16), hid_size, n_levels, kernel_size, dropout=dropout, model='Gen')\n",
    "        self.decoder = nn.Linear(hid_size, dic_size)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        emb = self.emb(input)\n",
    "        pro = self.dense0(label).unsqueeze(-1).expand(-1, -1, emb.size(1))\n",
    "        y = self.encoder(torch.cat((emb.transpose(1, 2), pro), dim=1))\n",
    "        o = self.decoder(y.transpose(1, 2))\n",
    "        return o.contiguous()\n",
    "\n",
    "\n",
    "class GlobalAttentionPooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, gate_nn, feat_nn=None):\n",
    "        super(GlobalAttentionPooling, self).__init__()\n",
    "        self.gate_nn = gate_nn\n",
    "        self.feat_nn = feat_nn\n",
    "        self.gate, self.feat = (None, None)\n",
    "\n",
    "    def forward(self, graph, feat):\n",
    "        \n",
    "        with graph.local_scope():\n",
    "            gate = self.gate_nn(feat)\n",
    "            assert gate.shape[-1] == 1, \"The output of gate_nn should have size 1 at the last axis.\"\n",
    "            feat = self.feat_nn(feat) if self.feat_nn else feat\n",
    "\n",
    "            graph.ndata['gate'] = gate\n",
    "            gate = softmax_nodes(graph, 'gate')\n",
    "            graph.ndata.pop('gate')\n",
    "\n",
    "            self.gate, self.feat = gate, feat \n",
    "\n",
    "            graph.ndata['r'] = feat * gate\n",
    "            readout = sum_nodes(graph, 'r')\n",
    "            graph.ndata.pop('r')\n",
    "\n",
    "            return readout\n",
    "\n",
    "\n",
    "class PRE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, h_size, emb_h, dim):\n",
    "        super(PRE, self).__init__()\n",
    "\n",
    "        self.emb_h = NNet(h_size,emb_h,hide=(64,128,64))\n",
    "        self.convs = GAT(emb_h,hidden_feats=[dim,dim*2,dim*2,dim*2,dim*2,dim],num_heads=[8,4,4,4,4,1],\n",
    "                         agg_modes=['flatten','flatten','flatten','flatten','flatten','mean'])\n",
    "\n",
    "        self.global_atten = GlobalAttentionPooling(NNet(dim, 1, hide=(32,)), NNet(dim, 1, hide=(32,)))\n",
    "\n",
    "    def forward(self, bg, feats):\n",
    "        feats = self.emb_h(feats.float())\n",
    "        feats = self.convs(bg,feats)\n",
    "        x = self.global_atten(bg,feats)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8447fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import config\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from model import PRE\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import dgl\n",
    "from gpuinfo import GPUInfo\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def train(train_iter):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_iter:\n",
    "        smiles, bg, labels, masks = data\n",
    "        bg, labels, masks = bg.to(device), labels[:,args.property_n].to(device), masks.to(device)#\n",
    "        node_feats = bg.ndata.pop('atomic').to(device)\n",
    "        # edge_feats = bg.edata.pop('e').to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(bg, node_feats)\n",
    "        loss = (F.mse_loss(outputs,labels)* (masks != 0).float()).mean()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()  \n",
    "\n",
    "    return total_loss/len(train_iter)\n",
    "\n",
    "\n",
    "def evaluate(data_iter):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        for data in data_iter:\n",
    "            smiles, bg, labels, masks = data\n",
    "            bg, labels, masks = bg.to(device), labels[:,args.property_n].to(device), masks.to(device)#\n",
    "            node_feats = bg.ndata.pop('atomic').to(device)\n",
    "            # edge_feats = bg.edata.pop('e').to(device)\n",
    "            \n",
    "            outputs = model(bg, node_feats)\n",
    "            loss = (F.mse_loss(outputs,labels)* (masks != 0).float()).mean()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss/len(data_iter)\n",
    "\n",
    "\n",
    "def collate_molgraphs(data):\n",
    "\n",
    "    smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "\n",
    "    bg = dgl.batch(graphs)\n",
    "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "\n",
    "    if masks is None:\n",
    "        masks = torch.ones(labels.shape)\n",
    "    else:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "\n",
    "    return smiles, bg, labels, masks\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Prediction Modeling',parents=[config.parser])\n",
    "    parser.add_argument('--save_name', type=str, default='reo_pre0.pt',help='the name of save model')\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    torch.manual_seed(1024)\n",
    "    torch.cuda.manual_seed(1024)\n",
    "\n",
    "    train_set, vali_set, test_set = torch.load(\"data/opv_graph-s.pt\")\n",
    "    train_iter = DataLoader(train_set, args.batch_size, shuffle=True, collate_fn=collate_molgraphs)\n",
    "    val_iter = DataLoader(vali_set, args.batch_size, shuffle=False, collate_fn=collate_molgraphs)\n",
    "    test_iter = DataLoader(test_set,1, shuffle=False, collate_fn=collate_molgraphs)\n",
    "\n",
    "    model = PRE(h_size=args.h_size,emb_h=args.emb_h,dim=args.hid_size)\n",
    "    # model_pre = torch.load(\"init_model/results/lumo_pre0.pt\").state_dict()\n",
    "    # model.load_state_dict(model_pre, strict=False)\n",
    "\n",
    "    # if torch.cuda.is_available():\n",
    "    #     available_device=GPUInfo.check_empty()\n",
    "    #     percent,memory=GPUInfo.gpu_usage()\n",
    "    #     # min_memory=memory.index(min([memory[i] for i in available_device]))\n",
    "    #     min_percent=percent.index(min([percent[i] for i in available_device]))\n",
    "    #     torch.cuda.set_device(min_percent)#min_memory\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    optimizer = getattr(optim, args.optim)(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    best_vloss =1000\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(1,args.epochs+1):\n",
    "        start_time = time.time()\n",
    "        train_loss = train(train_iter)\n",
    "        val_loss = evaluate(val_iter)\n",
    "\n",
    "        print('-' * 80)\n",
    "        print('epoch: {:4d} | time: {:4.4f}s | train loss: {:4.6f} | valid loss: {:4.6f}'.format\n",
    "              (epoch, time.time() - start_time, train_loss, val_loss))\n",
    "\n",
    "        writer.add_scalar('Train Loss', train_loss, epoch)\n",
    "        writer.add_scalar('Valid Loss', val_loss, epoch)\n",
    "        \n",
    "        if val_loss < best_vloss:\n",
    "            print('-' * 80)\n",
    "            print('Save model!')\n",
    "            torch.save(model, 'results/'+args.save_name)\n",
    "            best_vloss = val_loss\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    model = torch.load(\"results/\"+args.save_name)\n",
    "    test_loss = evaluate(test_iter)\n",
    "    print('=' * 40)\n",
    "    print('End of training | test MSE {:4.6f}'.format(test_loss))\n",
    "    print('=' * 40)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        Targets = []\n",
    "        Outputs = []\n",
    "        for data in test_iter:\n",
    "            smiles, bg, labels, masks = data\n",
    "            bg, labels, masks = bg.to(device), labels[:,args.property_n].to(device), masks.to(device)#\n",
    "            node_feats = bg.ndata.pop('atomic')\n",
    "            # edge_feats = bg.edata.pop('e').to(device)\n",
    "            \n",
    "            outputs = model(bg, node_feats.to(device))\n",
    "            loss = (F.l1_loss(outputs,labels)*(masks != 0).float()).mean()\n",
    "            total_loss += loss.item()\n",
    "            Outputs += [outputs.item()]\n",
    "            Targets += [labels.item()]\n",
    "\n",
    "    print('Test MAE {:4.6f} | REO_pearsonr {:4.6f}'.format(total_loss/len(test_iter),pearsonr(Targets, Outputs)[0]))\n",
    "\n",
    "    plt.figure(figsize=(5, 5),dpi=500)\n",
    "    plt.scatter(Targets,Outputs)\n",
    "    plt.axis([0.2,0.7, 0.2,0.7])\n",
    "    plt.title('reorganization energy',fontsize=12)\n",
    "    plt.xlabel('reo_real(eV)',fontsize=10)\n",
    "    plt.ylabel('reo_pre(eV)',fontsize=10)\n",
    "    plt.savefig(\"re0.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
